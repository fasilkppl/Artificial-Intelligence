{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences # to add padding\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Embedding # to get word embeddedings\n",
    "\n",
    "reviews = ['nice food',\n",
    "        'amazing restaurant',\n",
    "        'too good',\n",
    "        'just loved it!',\n",
    "        'will go again',\n",
    "        'horrible food',\n",
    "        'never go there',\n",
    "        'poor service',\n",
    "        'poor quality',\n",
    "        'needs improvement']\n",
    "\n",
    "sentiment = np.array([1,1,1,1,1,0,0,0,0,0]) # first 5 are positive reviews and, next 5 are negetive reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 2]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is basically one-hot encoding of vectors.\n",
    "# Convert the words in the text into one-hot vectors using a vocabulary size of 30.\n",
    "one_hot(\"amazing restaurant\",30)\n",
    "\n",
    "# Splits the text into words, Applies a hashing function to each word\n",
    "# Maps each word to an integer index in the range 1 to 30 in this case\n",
    "# in this case it returned 3 for the word 'amazing' and 2 for the word 'restaurant'\n",
    "# internally these will be converted to a vector with only 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6, 21], [3, 2], [23, 13], [21, 9, 12], [4, 16, 17], [7, 21], [21, 16, 13], [27, 7], [27, 23], [20, 19]]\n"
     ]
    }
   ],
   "source": [
    "# the above code is only for that 2 words 'amazing' and 'restaurant'\n",
    "# now i wanna do the same for entire reviews list.\n",
    "vocab_size = 30\n",
    "encoded_reviews = [one_hot(d, vocab_size) for d in reviews]\n",
    "print(encoded_reviews)\n",
    "\n",
    "# now this encoded unique values to every words in all reviews. internally coverted them to vectors with only 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6 21  0  0]\n",
      " [ 3  2  0  0]\n",
      " [23 13  0  0]\n",
      " [21  9 12  0]\n",
      " [ 4 16 17  0]\n",
      " [ 7 21  0  0]\n",
      " [21 16 13  0]\n",
      " [27  7  0  0]\n",
      " [27 23  0  0]\n",
      " [20 19  0  0]]\n"
     ]
    }
   ],
   "source": [
    "# as you can see, some reviews are 3 words long and some are 2 words long\n",
    "# so we need to add padding (append zeros at the end) to the review containing 2 words or less.\n",
    "# this is done to match the length of all words.\n",
    "# here we are just coverting all reviews to reviews containing 4 words each.\n",
    "# i can also give max_length to 3 as well, as my max number of words in each review is 3.\n",
    "max_length = 4\n",
    "# passing our one-hot encoded vectors (we modified above) to pad_sequences() function\n",
    "padded_reviews = pad_sequences(encoded_reviews, maxlen=max_length, padding='post') # padding='post' means append in the end\n",
    "print(padded_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embeded_vector_size = 5\n",
    "# creating in model in a sequential way.\n",
    "model = Sequential()\n",
    "# adding my first layer using Embedding that i imported earlier.\n",
    "# just assigning embeded_vector_size, input length as my max length, also a name=\"embedding\" which i can use earlier.\n",
    "model.add(Embedding(vocab_size, embeded_vector_size, input_length=max_length,name=\"embedding\"))\n",
    "# flatten those embedding vectors\n",
    "model.add(Flatten())\n",
    "# then we add 1 neuron output layer with sigmoid activation function.\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = padded_reviews # assigning my padded out to X\n",
    "y = sentiment # this assigned above - sentiment = np.array([1,1,1,1,1,0,0,0,0,0]), first 5 are positive reviews and, next 5 are negetive reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " embedding (Embedding)       (None, 4, 5)              150       \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 20)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 171 (684.00 Byte)\n",
      "Trainable params: 171 (684.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# im using loss='binary_crossentropy' because my out is either 1 or 0, ie, positive or negetive.\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x121d67b8ee0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model\n",
    "model.fit(X, y, epochs=50, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 216ms/step - loss: 0.6266 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(X, y)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.02525621,  0.03310835, -0.04979291,  0.08472165, -0.07781053],\n",
       "       [ 0.04140401,  0.04554696,  0.0142842 , -0.04178649, -0.00398065],\n",
       "       [ 0.06319761,  0.05469593,  0.0916774 ,  0.0691436 ,  0.04260057],\n",
       "       [-0.06141017, -0.08621411,  0.09558144,  0.0996749 ,  0.08924381],\n",
       "       [-0.04635488, -0.05954704,  0.05853776,  0.06600622,  0.04873991],\n",
       "       [-0.02964232,  0.03196828, -0.00283191, -0.0307832 , -0.01620314],\n",
       "       [-0.08496669, -0.00986668,  0.04644088,  0.02344804,  0.09203819],\n",
       "       [ 0.07469345, -0.05078626, -0.06861632, -0.0985084 , -0.04210273],\n",
       "       [ 0.03263037, -0.04552471,  0.0490703 , -0.04830383,  0.04754186],\n",
       "       [ 0.0839077 ,  0.01336703,  0.01359792,  0.00332644,  0.08739428],\n",
       "       [ 0.03857725, -0.00233979, -0.03952513,  0.01684934, -0.00230075],\n",
       "       [ 0.00371198,  0.02089468,  0.02805854, -0.02096107, -0.02942134],\n",
       "       [-0.043928  , -0.0695344 ,  0.05290475, -0.03965294,  0.07815702],\n",
       "       [ 0.0530896 ,  0.07526175,  0.06364192,  0.06548808, -0.00441356],\n",
       "       [-0.011057  ,  0.03597881, -0.01907492, -0.01216381, -0.00072541],\n",
       "       [ 0.00153974,  0.02855656,  0.04716501, -0.04063268, -0.04347694],\n",
       "       [-0.00645382, -0.06732691, -0.04259013, -0.0171776 , -0.0064818 ],\n",
       "       [-0.02008514, -0.04649075,  0.02657702, -0.00759508,  0.03217838],\n",
       "       [ 0.04653091,  0.01839671,  0.01621416,  0.00903177, -0.01970378],\n",
       "       [-0.06598553, -0.08443683, -0.02977161, -0.02462223, -0.03847231],\n",
       "       [ 0.02945402,  0.06902123, -0.09467924, -0.07375921, -0.01097251],\n",
       "       [ 0.01229456,  0.00414015, -0.00074317, -0.02520862,  0.07301504],\n",
       "       [ 0.00692225, -0.01679027,  0.00067918, -0.04208906, -0.038528  ],\n",
       "       [-0.09562342, -0.03184757,  0.08758849,  0.09977049, -0.04770611],\n",
       "       [-0.04698386, -0.03427708,  0.02981142, -0.00237223,  0.01685358],\n",
       "       [ 0.0118807 , -0.00572139,  0.00682155,  0.02962104,  0.03776969],\n",
       "       [ 0.02248241, -0.00777472, -0.0082614 , -0.02012764,  0.04885497],\n",
       "       [ 0.09230021,  0.09445493, -0.09763067, -0.01773426, -0.05107886],\n",
       "       [ 0.0387984 , -0.04254977, -0.03860803,  0.01995048, -0.0257709 ],\n",
       "       [-0.00990908, -0.04293745, -0.0402467 , -0.0065647 ,  0.00046468]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is our embedded wording.\n",
    "# we use name we assigned earlier.\n",
    "# this is the weight of all vectors (features)\n",
    "weights = model.get_layer('embedding').get_weights()[0]\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(weights) # as my vocabilary size was 30, my weights is also 30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0530896 ,  0.07526175,  0.06364192,  0.06548808, -0.00441356],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.04635488, -0.05954704,  0.05853776,  0.06600622,  0.04873991],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00645382, -0.06732691, -0.04259013, -0.0171776 , -0.0064818 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[16]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
