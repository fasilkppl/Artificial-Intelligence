{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n(1) Train a hand written digits model\\n\\n(2) Export to a disk and check the size of that model\\n\\n(3) Use two techniques for quantization (1) post training quantization (3) quantization aware training'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "(1) Train a hand written digits model\n",
    "\n",
    "(2) Export to a disk and check the size of that model\n",
    "\n",
    "(3) Use two techniques for quantization (1) post training quantization (3) quantization aware training'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test, train splt using hand written dataset\n",
    "(X_train, y_train) , (X_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1f764353460>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.matshow(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train / 255\n",
    "X_test = X_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_flattened = X_train.reshape(len(X_train), 28*28)\n",
    "X_test_flattened = X_test.reshape(len(X_test), 28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_flattened.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Flatten layer so that we don't have to call .reshape on input dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 7s 3ms/step - loss: 0.2745 - accuracy: 0.9221\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1224 - accuracy: 0.9640\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0856 - accuracy: 0.9738\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0659 - accuracy: 0.9804\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0523 - accuracy: 0.9833\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f76550b250>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(100, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0812 - accuracy: 0.9748\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.08122190088033676, 0.9747999906539917]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./saved_model/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"./saved_model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) Post training quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting model that have finished training to lite version using tf.lite converter\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(\"./saved_model\") # model loaded from disk\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_saved_model(\"./saved_model\") # model loaded from disk\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT] # add this optimization line.\n",
    "tflite_quant_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "320040"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# size of model without  quantization.\n",
    "len(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84912"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# size of model with quantization.\n",
    "len(tflite_quant_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can see above that quantizated model is 1/4th the size of a non quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model to a file.\n",
    "with open(\"tflite_model.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model to a file.\n",
    "with open(\"tflite_quant_model.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_quant_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once you have above files saved to a disk, check their sizes. Quantized model will be having less size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2) Quantization aware training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " quantize_layer (QuantizeLay  (None, 28, 28)           3         \n",
      " er)                                                             \n",
      "                                                                 \n",
      " quant_flatten_1 (QuantizeWr  (None, 784)              1         \n",
      " apperV2)                                                        \n",
      "                                                                 \n",
      " quant_dense_2 (QuantizeWrap  (None, 100)              78505     \n",
      " perV2)                                                          \n",
      "                                                                 \n",
      " quant_dense_3 (QuantizeWrap  (None, 10)               1015      \n",
      " perV2)                                                          \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 79,524\n",
      "Trainable params: 79,510\n",
      "Non-trainable params: 14\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "quantize_model = tfmot.quantization.keras.quantize_model # import quantization from tensor flow\n",
    "\n",
    "# applying quantize_model function on our original training tf model.\n",
    "q_aware_model = quantize_model(model) \n",
    "\n",
    "# `quantize_model` requires a recompile. (this model is already compiled, we are recompiling it.)\n",
    "q_aware_model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "q_aware_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 10s 4ms/step - loss: 0.0436 - accuracy: 0.9858\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f776139cf0>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_aware_model.fit(X_train, y_train, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0830 - accuracy: 0.9751\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.08296483010053635, 0.9750999808311462]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_aware_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as flatten_1_layer_call_fn, flatten_1_layer_call_and_return_conditional_losses, dense_2_layer_call_fn, dense_2_layer_call_and_return_conditional_losses, dense_3_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\fasil\\AppData\\Local\\Temp\\tmpchxn31qo\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\fasil\\AppData\\Local\\Temp\\tmpchxn31qo\\assets\n",
      "c:\\Users\\fasil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py:766: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n"
     ]
    }
   ],
   "source": [
    "# finally converting our fine tuned model to lite version using tf.lite\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model) # # model loaded keras \n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "tflite_qaware_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82776"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tflite_qaware_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tflite_qaware_model.tflite\", 'wb') as f:\n",
    "    f.write(tflite_qaware_model)\n",
    "# model extension here is .tflite"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
